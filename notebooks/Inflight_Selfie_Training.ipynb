{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úàÔ∏è Inflight Selfie Generator - Training Notebook\n",
        "\n",
        "This notebook trains an AI model to generate realistic inflight selfies using:\n",
        "- **IP-Adapter-FaceID** for face-preserving image generation\n",
        "- **Stable Diffusion XL** as the base model\n",
        "- **TinyLlama** for intelligent scene planning\n",
        "- **InsightFace** for face embedding extraction\n",
        "\n",
        "## Hardware Requirements\n",
        "- Google Colab with T4 GPU (FREE tier)\n",
        "- ~15GB VRAM\n",
        "\n",
        "## Training Time\n",
        "- IP-Adapter setup: ~10 minutes\n",
        "- TinyLlama fine-tuning: ~20-30 minutes\n",
        "- Total: ~40 minutes on T4"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1: Environment Setup"
      ],
      "metadata": {
        "id": "phase1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install Core Dependencies\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q diffusers transformers accelerate safetensors\n",
        "!pip install -q opencv-python-headless pillow\n",
        "!pip install -q insightface onnxruntime-gpu\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q peft  # For LoRA fine-tuning\n",
        "!pip install -q bitsandbytes  # For 4-bit quantization\n",
        "!pip install -q datasets trl  # For training\n",
        "!pip install -q gradio  # For testing interface\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Verify GPU\n",
        "import torch\n",
        "\n",
        "print(\"üîç Checking GPU availability...\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è WARNING: No GPU detected! This notebook requires a GPU.\")\n",
        "    print(\"Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")"
      ],
      "metadata": {
        "id": "verify_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Download Required Models\n",
        "from huggingface_hub import hf_hub_download, snapshot_download\n",
        "import os\n",
        "\n",
        "print(\"üì• Downloading models...\")\n",
        "\n",
        "# Create model directories\n",
        "os.makedirs(\"models/ip-adapter-faceid\", exist_ok=True)\n",
        "os.makedirs(\"models/insightface/models/antelopev2\", exist_ok=True)\n",
        "\n",
        "# Download IP-Adapter-FaceID models\n",
        "print(\"  Downloading IP-Adapter-FaceID...\")\n",
        "hf_hub_download(\n",
        "    repo_id=\"h94/IP-Adapter-FaceID\",\n",
        "    filename=\"ip-adapter-faceid_sdxl.bin\",\n",
        "    local_dir=\"./models/ip-adapter-faceid\"\n",
        ")\n",
        "\n",
        "hf_hub_download(\n",
        "    repo_id=\"h94/IP-Adapter-FaceID\",\n",
        "    filename=\"ip-adapter-faceid-plusv2_sdxl.bin\",\n",
        "    local_dir=\"./models/ip-adapter-faceid\"\n",
        ")\n",
        "\n",
        "# Download InsightFace models for face embedding\n",
        "print(\"  Downloading InsightFace models...\")\n",
        "insightface_models = [\n",
        "    \"1k3d68.onnx\",\n",
        "    \"2d106det.onnx\",\n",
        "    \"genderage.onnx\",\n",
        "    \"glintr100.onnx\",\n",
        "    \"scrfd_10g_bnkps.onnx\"\n",
        "]\n",
        "\n",
        "for model_file in insightface_models:\n",
        "    !wget -q -O models/insightface/models/antelopev2/{model_file} \\\n",
        "        https://huggingface.co/datasets/Gourieff/ReActor/resolve/main/models/insightface/models/antelopev2/{model_file}\n",
        "\n",
        "print(\"‚úÖ All models downloaded!\")"
      ],
      "metadata": {
        "id": "download_models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2: IP-Adapter-FaceID Pipeline"
      ],
      "metadata": {
        "id": "phase2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Core Pipeline Implementation\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from diffusers import StableDiffusionXLPipeline, DDIMScheduler\n",
        "from insightface.app import FaceAnalysis\n",
        "from typing import List, Union\n",
        "\n",
        "class InflightSelfiePipeline:\n",
        "    \"\"\"IP-Adapter-FaceID pipeline for generating inflight selfies.\"\"\"\n",
        "    \n",
        "    def __init__(self, device=\"cuda\"):\n",
        "        self.device = device\n",
        "        self.face_analyzer = None\n",
        "        self.pipe = None\n",
        "        \n",
        "        print(\"üöÄ Initializing Inflight Selfie Pipeline...\")\n",
        "        self.setup_face_analyzer()\n",
        "        self.setup_diffusion_pipeline()\n",
        "        print(\"‚úÖ Pipeline ready!\")\n",
        "    \n",
        "    def setup_face_analyzer(self):\n",
        "        \"\"\"Initialize InsightFace for face embedding extraction.\"\"\"\n",
        "        print(\"  Loading face analyzer...\")\n",
        "        self.face_analyzer = FaceAnalysis(\n",
        "            name='antelopev2',\n",
        "            root='./models/insightface',\n",
        "            providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        "        )\n",
        "        self.face_analyzer.prepare(ctx_id=0, det_size=(640, 640))\n",
        "        print(\"    ‚úì Face analyzer ready\")\n",
        "    \n",
        "    def setup_diffusion_pipeline(self):\n",
        "        \"\"\"Initialize SDXL with IP-Adapter-FaceID.\"\"\"\n",
        "        print(\"  Loading SDXL pipeline...\")\n",
        "        \n",
        "        # Load base SDXL\n",
        "        self.pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "            \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "            torch_dtype=torch.float16,\n",
        "            variant=\"fp16\",\n",
        "        ).to(self.device)\n",
        "        \n",
        "        # Use efficient scheduler\n",
        "        self.pipe.scheduler = DDIMScheduler.from_config(self.pipe.scheduler.config)\n",
        "        \n",
        "        # Load IP-Adapter-FaceID\n",
        "        print(\"    Loading IP-Adapter-FaceID...\")\n",
        "        self.pipe.load_ip_adapter(\n",
        "            \"h94/IP-Adapter-FaceID\",\n",
        "            subfolder=None,\n",
        "            weight_name=\"ip-adapter-faceid_sdxl.bin\",\n",
        "        )\n",
        "        \n",
        "        # Enable memory optimizations for T4\n",
        "        self.pipe.enable_model_cpu_offload()\n",
        "        self.pipe.enable_vae_slicing()\n",
        "        \n",
        "        print(\"    ‚úì SDXL + IP-Adapter-FaceID ready\")\n",
        "    \n",
        "    def extract_face_embedding(self, image: Union[str, np.ndarray, Image.Image]) -> np.ndarray:\n",
        "        \"\"\"Extract face embedding from image.\"\"\"\n",
        "        # Convert to numpy array\n",
        "        if isinstance(image, str):\n",
        "            img = cv2.imread(image)\n",
        "        elif isinstance(image, Image.Image):\n",
        "            img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "        else:\n",
        "            img = image\n",
        "        \n",
        "        faces = self.face_analyzer.get(img)\n",
        "        \n",
        "        if not faces:\n",
        "            raise ValueError(\"No face detected in image\")\n",
        "        \n",
        "        # Return the embedding of the largest face\n",
        "        face = max(faces, key=lambda x: (x.bbox[2] - x.bbox[0]) * (x.bbox[3] - x.bbox[1]))\n",
        "        return face.embedding\n",
        "    \n",
        "    def extract_face_embeddings_multi(self, images: List[Union[str, np.ndarray, Image.Image]]) -> np.ndarray:\n",
        "        \"\"\"Extract and average embeddings from multiple images.\"\"\"\n",
        "        embeddings = []\n",
        "        \n",
        "        for img in images:\n",
        "            try:\n",
        "                emb = self.extract_face_embedding(img)\n",
        "                embeddings.append(emb)\n",
        "            except ValueError as e:\n",
        "                print(f\"    ‚ö†Ô∏è Warning: {e}\")\n",
        "                continue\n",
        "        \n",
        "        if not embeddings:\n",
        "            raise ValueError(\"No faces detected in any images\")\n",
        "        \n",
        "        # Average the embeddings\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    \n",
        "    def generate_selfie(\n",
        "        self,\n",
        "        person1_images: List[Union[str, np.ndarray, Image.Image]],\n",
        "        person2_images: List[Union[str, np.ndarray, Image.Image]],\n",
        "        prompt: str,\n",
        "        negative_prompt: str = None,\n",
        "        num_inference_steps: int = 30,\n",
        "        guidance_scale: float = 7.5,\n",
        "        ip_adapter_scale: float = 0.6,\n",
        "        seed: int = None,\n",
        "    ) -> Image.Image:\n",
        "        \"\"\"\n",
        "        Generate inflight selfie with two people.\n",
        "        \n",
        "        Args:\n",
        "            person1_images: List of images for person 1 (paths, arrays, or PIL Images)\n",
        "            person2_images: List of images for person 2\n",
        "            prompt: Scene description\n",
        "            negative_prompt: What to avoid\n",
        "            num_inference_steps: Diffusion steps (higher = better quality, slower)\n",
        "            guidance_scale: CFG scale (higher = more prompt adherence)\n",
        "            ip_adapter_scale: Identity preservation strength (0-1)\n",
        "            seed: Random seed for reproducibility\n",
        "        \n",
        "        Returns:\n",
        "            Generated PIL Image\n",
        "        \"\"\"\n",
        "        \n",
        "        # Extract face embeddings\n",
        "        print(\"  Extracting face embeddings...\")\n",
        "        emb1 = self.extract_face_embeddings_multi(person1_images)\n",
        "        emb2 = self.extract_face_embeddings_multi(person2_images)\n",
        "        \n",
        "        # Combine embeddings (weighted average)\n",
        "        # This is a simplified approach - for production, consider:\n",
        "        # 1. Generating two separate images and compositing\n",
        "        # 2. Using IP-Adapter-FaceID-Plus for multi-face support\n",
        "        combined_emb = (emb1 + emb2) / 2\n",
        "        face_emb_tensor = torch.tensor(combined_emb, dtype=torch.float16).unsqueeze(0).to(self.device)\n",
        "        \n",
        "        # Default negative prompt\n",
        "        if negative_prompt is None:\n",
        "            negative_prompt = (\n",
        "                \"ugly, blurry, low quality, distorted face, bad anatomy, \"\n",
        "                \"deformed, disfigured, watermark, text, oversaturated, \"\n",
        "                \"extra limbs, missing limbs, floating limbs, mutation, \"\n",
        "                \"duplicate faces, bad eyes, asymmetric eyes\"\n",
        "            )\n",
        "        \n",
        "        # Set IP-Adapter scale\n",
        "        self.pipe.set_ip_adapter_scale(ip_adapter_scale)\n",
        "        \n",
        "        # Set seed if provided\n",
        "        generator = None\n",
        "        if seed is not None:\n",
        "            generator = torch.Generator(device=self.device).manual_seed(seed)\n",
        "        \n",
        "        # Generate\n",
        "        print(\"  Generating image...\")\n",
        "        result = self.pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            ip_adapter_image_embeds=[face_emb_tensor],\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            generator=generator,\n",
        "            height=1024,\n",
        "            width=1024,\n",
        "        )\n",
        "        \n",
        "        print(\"  ‚úÖ Generation complete!\")\n",
        "        return result.images[0]\n",
        "\n",
        "\n",
        "# Initialize the pipeline\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "pipeline = InflightSelfiePipeline()\n",
        "print(\"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "core_pipeline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 3: TinyLlama Scene Planner"
      ],
      "metadata": {
        "id": "phase3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Scene Planning Dataset\n",
        "import json\n",
        "\n",
        "# Training data for scene planning\n",
        "scene_planning_data = [\n",
        "    {\n",
        "        \"instruction\": \"Two friends taking a sunset window selfie flying to Dubai\",\n",
        "        \"output\": json.dumps({\n",
        "            \"prompt\": \"two friends taking selfie in airplane window seat, golden sunset visible through window, Dubai skyline approaching in distance, warm golden lighting, happy excited expressions, economy cabin, airplane interior, high quality photo, realistic, detailed faces\",\n",
        "            \"negative_prompt\": \"ugly, blurry, distorted, bad anatomy, deformed faces\",\n",
        "            \"ip_adapter_scale\": 0.65,\n",
        "            \"guidance_scale\": 7.5,\n",
        "            \"scene_type\": \"sunset_window\",\n",
        "        }, indent=2)\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Business class celebration with champagne\",\n",
        "        \"output\": json.dumps({\n",
        "            \"prompt\": \"two people taking selfie in business class airplane cabin, champagne glasses in hand, celebrating, luxury wide seats, premium cabin interior, soft ambient lighting, joyful expressions, high quality photo, detailed, realistic\",\n",
        "            \"negative_prompt\": \"ugly, blurry, economy class, cheap, low quality\",\n",
        "            \"ip_adapter_scale\": 0.6,\n",
        "            \"guidance_scale\": 7.0,\n",
        "            \"scene_type\": \"business_celebration\",\n",
        "        }, indent=2)\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Night flight with city lights below\",\n",
        "        \"output\": json.dumps({\n",
        "            \"prompt\": \"two people taking selfie in airplane at night, city lights visible through window below, dim cabin lighting with city glow on faces, amazed expressions, window seat, high quality photo, realistic, atmospheric\",\n",
        "            \"negative_prompt\": \"daylight, bright, ugly, blurry, bad lighting\",\n",
        "            \"ip_adapter_scale\": 0.7,\n",
        "            \"guidance_scale\": 7.5,\n",
        "            \"scene_type\": \"night_city\",\n",
        "        }, indent=2)\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Morning flight over clouds\",\n",
        "        \"output\": json.dumps({\n",
        "            \"prompt\": \"two people taking selfie in airplane window seat, fluffy white clouds visible outside, bright morning sunlight, fresh morning mood, happy smiling faces, economy cabin, high quality photo, realistic\",\n",
        "            \"negative_prompt\": \"dark, night, ugly, blurry, bad quality\",\n",
        "            \"ip_adapter_scale\": 0.65,\n",
        "            \"guidance_scale\": 7.0,\n",
        "            \"scene_type\": \"morning_clouds\",\n",
        "        }, indent=2)\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"First class luxury experience\",\n",
        "        \"output\": json.dumps({\n",
        "            \"prompt\": \"two people taking selfie in first class airplane suite, spacious luxury cabin, premium amenities visible, elegant lighting, sophisticated expressions, high-end travel experience, high quality photo, detailed, realistic\",\n",
        "            \"negative_prompt\": \"cheap, low quality, economy, cramped, ugly\",\n",
        "            \"ip_adapter_scale\": 0.6,\n",
        "            \"guidance_scale\": 7.5,\n",
        "            \"scene_type\": \"first_class\",\n",
        "        }, indent=2)\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Takeoff excitement from runway\",\n",
        "        \"output\": json.dumps({\n",
        "            \"prompt\": \"two friends taking selfie during airplane takeoff, runway visible through window, excited nervous expressions, beginning of journey mood, window seat, high quality photo, realistic, dynamic moment\",\n",
        "            \"negative_prompt\": \"calm, boring, ugly, blurry, static\",\n",
        "            \"ip_adapter_scale\": 0.7,\n",
        "            \"guidance_scale\": 7.5,\n",
        "            \"scene_type\": \"takeoff\",\n",
        "        }, indent=2)\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Red eye flight tired but happy\",\n",
        "        \"output\": json.dumps({\n",
        "            \"prompt\": \"two travelers taking selfie during red eye flight, tired but happy expressions, blankets visible, dim cabin lighting, night atmosphere, cozy travel mood, high quality photo, realistic\",\n",
        "            \"negative_prompt\": \"energetic, bright, ugly, blurry\",\n",
        "            \"ip_adapter_scale\": 0.65,\n",
        "            \"guidance_scale\": 7.0,\n",
        "            \"scene_type\": \"red_eye\",\n",
        "        }, indent=2)\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Landing with destination airport view\",\n",
        "        \"output\": json.dumps({\n",
        "            \"prompt\": \"two people taking selfie during airplane landing, destination airport visible through window, excited arrival expressions, end of journey celebration, window seat, high quality photo, realistic, arrival mood\",\n",
        "            \"negative_prompt\": \"departure, ugly, blurry, bad quality\",\n",
        "            \"ip_adapter_scale\": 0.65,\n",
        "            \"guidance_scale\": 7.5,\n",
        "            \"scene_type\": \"landing\",\n",
        "        }, indent=2)\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Ocean view tropical destination\",\n",
        "        \"output\": json.dumps({\n",
        "            \"prompt\": \"two friends taking selfie in airplane, tropical ocean and islands visible through window, bright blue water below, vacation excitement mood, happy expressions, window seat, high quality photo, realistic, travel adventure\",\n",
        "            \"negative_prompt\": \"ugly, blurry, dark, winter, mountains\",\n",
        "            \"ip_adapter_scale\": 0.65,\n",
        "            \"guidance_scale\": 7.0,\n",
        "            \"scene_type\": \"tropical\",\n",
        "        }, indent=2)\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Business trip colleagues professional\",\n",
        "        \"output\": json.dumps({\n",
        "            \"prompt\": \"two business colleagues taking selfie in airplane, professional friendly expressions, business casual attire, business travel atmosphere, modern cabin, high quality photo, realistic, professional mood\",\n",
        "            \"negative_prompt\": \"casual, party, ugly, blurry, unprofessional\",\n",
        "            \"ip_adapter_scale\": 0.6,\n",
        "            \"guidance_scale\": 7.5,\n",
        "            \"scene_type\": \"business\",\n",
        "        }, indent=2)\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ Created {len(scene_planning_data)} training examples\")"
      ],
      "metadata": {
        "id": "scene_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Fine-tune TinyLlama for Scene Planning\n",
        "!pip install -q unsloth\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import json\n",
        "\n",
        "print(\"üîß Fine-tuning TinyLlama for scene planning...\")\n",
        "\n",
        "# Load TinyLlama with 4-bit quantization\n",
        "print(\"  Loading TinyLlama...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/tinyllama-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "print(\"  Adding LoRA adapters...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=True,\n",
        ")\n",
        "\n",
        "# Format training data\n",
        "def format_prompt(item):\n",
        "    return f\"\"\"<|system|>\n",
        "You are an inflight selfie scene planner. Given a user's description, output a JSON configuration with optimal parameters for generating a realistic inflight selfie.\n",
        "<|user|>\n",
        "{item['instruction']}\n",
        "<|assistant|>\n",
        "{item['output']}\"\"\"\n",
        "\n",
        "formatted_data = [{\"text\": format_prompt(item)} for item in scene_planning_data]\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "\n",
        "print(f\"  Training on {len(dataset)} examples...\")\n",
        "\n",
        "# Train\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=1024,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./scene_planner_lora\",\n",
        "        num_train_epochs=10,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=5,\n",
        "        save_steps=50,\n",
        "        warmup_steps=10,\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "print(\"  Saving model...\")\n",
        "model.save_pretrained(\"scene_planner_lora\")\n",
        "tokenizer.save_pretrained(\"scene_planner_lora\")\n",
        "\n",
        "print(\"‚úÖ Scene planner trained and saved!\")"
      ],
      "metadata": {
        "id": "train_tinyllama"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Create Complete Pipeline with Scene Planner\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import os\n",
        "\n",
        "class CompleteInflightSelfiePipeline:\n",
        "    \"\"\"\n",
        "    Complete pipeline with:\n",
        "    1. TinyLlama Scene Planner - generates optimal parameters\n",
        "    2. InsightFace - extracts face embeddings\n",
        "    3. SDXL + IP-Adapter-FaceID - generates image\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        print(\"üöÄ Initializing Complete Inflight Selfie Pipeline...\")\n",
        "        self.setup_scene_planner()\n",
        "        self.setup_image_generator()\n",
        "        print(\"‚úÖ Complete pipeline ready!\\n\")\n",
        "    \n",
        "    def setup_scene_planner(self):\n",
        "        \"\"\"Load fine-tuned TinyLlama.\"\"\"\n",
        "        print(\"  Loading scene planner...\")\n",
        "        \n",
        "        if os.path.exists(\"scene_planner_lora\"):\n",
        "            # Load base model\n",
        "            base_model = AutoModelForCausalLM.from_pretrained(\n",
        "                \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "            \n",
        "            # Load LoRA weights\n",
        "            self.scene_planner = PeftModel.from_pretrained(base_model, \"scene_planner_lora\")\n",
        "            self.scene_tokenizer = AutoTokenizer.from_pretrained(\"scene_planner_lora\")\n",
        "            print(\"    ‚úì Loaded fine-tuned scene planner\")\n",
        "        else:\n",
        "            # Fallback to base model\n",
        "            self.scene_planner = AutoModelForCausalLM.from_pretrained(\n",
        "                \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map=\"auto\"\n",
        "            )\n",
        "            self.scene_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "            print(\"    ‚ö†Ô∏è Using base TinyLlama (not fine-tuned)\")\n",
        "    \n",
        "    def setup_image_generator(self):\n",
        "        \"\"\"Initialize the IP-Adapter-FaceID pipeline.\"\"\"\n",
        "        print(\"  Loading image generator...\")\n",
        "        self.image_pipeline = pipeline  # Use the already initialized pipeline\n",
        "        print(\"    ‚úì Image generator ready\")\n",
        "    \n",
        "    def plan_scene(self, user_prompt: str) -> dict:\n",
        "        \"\"\"Generate scene parameters from user prompt using TinyLlama.\"\"\"\n",
        "        \n",
        "        full_prompt = f\"\"\"<|system|>\n",
        "You are an inflight selfie scene planner. Given a user's description, output a JSON configuration with optimal parameters for generating a realistic inflight selfie.\n",
        "<|user|>\n",
        "{user_prompt}\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "        \n",
        "        inputs = self.scene_tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "        outputs = self.scene_planner.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=400,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=self.scene_tokenizer.eos_token_id\n",
        "        )\n",
        "        response = self.scene_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract JSON from response\n",
        "        try:\n",
        "            json_start = response.rfind(\"{\")\n",
        "            json_end = response.rfind(\"}\") + 1\n",
        "            \n",
        "            if json_start >= 0 and json_end > json_start:\n",
        "                params = json.loads(response[json_start:json_end])\n",
        "                return params\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Fallback defaults\n",
        "        return {\n",
        "            \"prompt\": f\"two people taking selfie in airplane, {user_prompt}, high quality photo, realistic, detailed faces\",\n",
        "            \"negative_prompt\": \"ugly, blurry, distorted, bad anatomy, deformed\",\n",
        "            \"ip_adapter_scale\": 0.65,\n",
        "            \"guidance_scale\": 7.5,\n",
        "        }\n",
        "    \n",
        "    def generate(\n",
        "        self,\n",
        "        user_prompt: str,\n",
        "        person1_images: List,\n",
        "        person2_images: List,\n",
        "        seed: int = None,\n",
        "    ) -> Image.Image:\n",
        "        \"\"\"\n",
        "        Complete generation pipeline.\n",
        "        \n",
        "        Args:\n",
        "            user_prompt: Natural language scene description\n",
        "            person1_images: List of images for person 1\n",
        "            person2_images: List of images for person 2\n",
        "            seed: Random seed for reproducibility\n",
        "        \n",
        "        Returns:\n",
        "            Generated PIL Image\n",
        "        \"\"\"\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üé¨ GENERATING INFLIGHT SELFIE\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Step 1: Plan the scene\n",
        "        print(\"\\nüìã Step 1: Planning scene...\")\n",
        "        scene_params = self.plan_scene(user_prompt)\n",
        "        print(f\"  Scene type: {scene_params.get('scene_type', 'custom')}\")\n",
        "        print(f\"  Prompt: {scene_params['prompt'][:80]}...\")\n",
        "        print(f\"  IP Scale: {scene_params.get('ip_adapter_scale', 0.65)}\")\n",
        "        print(f\"  CFG Scale: {scene_params.get('guidance_scale', 7.5)}\")\n",
        "        \n",
        "        # Step 2: Generate image\n",
        "        print(\"\\nüé® Step 2: Generating image...\")\n",
        "        result_image = self.image_pipeline.generate_selfie(\n",
        "            person1_images=person1_images,\n",
        "            person2_images=person2_images,\n",
        "            prompt=scene_params[\"prompt\"],\n",
        "            negative_prompt=scene_params.get(\"negative_prompt\"),\n",
        "            ip_adapter_scale=scene_params.get(\"ip_adapter_scale\", 0.65),\n",
        "            guidance_scale=scene_params.get(\"guidance_scale\", 7.5),\n",
        "            num_inference_steps=30,\n",
        "            seed=seed,\n",
        "        )\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"‚úÖ GENERATION COMPLETE!\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "        \n",
        "        return result_image\n",
        "\n",
        "\n",
        "# Initialize complete pipeline\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "complete_pipeline = CompleteInflightSelfiePipeline()\n",
        "print(\"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "complete_pipeline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 4: Testing & Demo"
      ],
      "metadata": {
        "id": "phase4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Upload Test Images and Generate\n",
        "from google.colab import files\n",
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "print(\"üì§ Upload face images for testing...\\n\")\n",
        "\n",
        "print(\"üë§ Person 1: Upload 1-5 face images\")\n",
        "uploaded_p1 = files.upload()\n",
        "person1_images = list(uploaded_p1.keys())\n",
        "print(f\"  ‚úì Uploaded {len(person1_images)} images\\n\")\n",
        "\n",
        "print(\"üë§ Person 2: Upload 1-5 face images\")\n",
        "uploaded_p2 = files.upload()\n",
        "person2_images = list(uploaded_p2.keys())\n",
        "print(f\"  ‚úì Uploaded {len(person2_images)} images\\n\")\n",
        "\n",
        "# Test scene descriptions\n",
        "test_scenes = [\n",
        "    \"Two friends taking a sunset selfie flying to Dubai\",\n",
        "    \"Business class celebration with champagne\",\n",
        "    \"Night flight with city lights below\",\n",
        "    \"Morning flight over fluffy clouds\",\n",
        "]\n",
        "\n",
        "print(\"\\nüé¨ Choose a scene or enter your own:\")\n",
        "for i, scene in enumerate(test_scenes, 1):\n",
        "    print(f\"  {i}. {scene}\")\n",
        "\n",
        "scene_choice = input(\"\\nEnter number (1-4) or custom description: \")\n",
        "\n",
        "if scene_choice.isdigit() and 1 <= int(scene_choice) <= 4:\n",
        "    user_prompt = test_scenes[int(scene_choice) - 1]\n",
        "else:\n",
        "    user_prompt = scene_choice\n",
        "\n",
        "print(f\"\\n‚ú® Generating: {user_prompt}\")\n",
        "\n",
        "# Generate!\n",
        "result = complete_pipeline.generate(\n",
        "    user_prompt=user_prompt,\n",
        "    person1_images=person1_images,\n",
        "    person2_images=person2_images,\n",
        "    seed=42,  # For reproducibility\n",
        ")\n",
        "\n",
        "# Display result\n",
        "print(\"\\nüì∏ Generated Image:\")\n",
        "display(result)\n",
        "\n",
        "# Save result\n",
        "result.save(\"generated_inflight_selfie.png\")\n",
        "print(\"\\nüíæ Saved as: generated_inflight_selfie.png\")\n",
        "\n",
        "# Download\n",
        "files.download(\"generated_inflight_selfie.png\")"
      ],
      "metadata": {
        "id": "test_generation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Interactive Gradio Demo\n",
        "import gradio as gr\n",
        "\n",
        "def generate_selfie_gradio(\n",
        "    person1_img1, person1_img2, person1_img3,\n",
        "    person2_img1, person2_img2, person2_img3,\n",
        "    prompt,\n",
        "    seed,\n",
        "):\n",
        "    \"\"\"Gradio interface for generation.\"\"\"\n",
        "    \n",
        "    # Collect uploaded images\n",
        "    p1_images = [img for img in [person1_img1, person1_img2, person1_img3] if img is not None]\n",
        "    p2_images = [img for img in [person2_img1, person2_img2, person2_img3] if img is not None]\n",
        "    \n",
        "    if not p1_images or not p2_images:\n",
        "        return None, \"‚ö†Ô∏è Please upload at least one image for each person!\"\n",
        "    \n",
        "    try:\n",
        "        result = complete_pipeline.generate(\n",
        "            user_prompt=prompt,\n",
        "            person1_images=p1_images,\n",
        "            person2_images=p2_images,\n",
        "            seed=seed if seed > 0 else None,\n",
        "        )\n",
        "        return result, \"‚úÖ Generation successful!\"\n",
        "    except Exception as e:\n",
        "        return None, f\"‚ùå Error: {str(e)}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=generate_selfie_gradio,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Person 1 - Image 1\"),\n",
        "        gr.Image(type=\"pil\", label=\"Person 1 - Image 2 (optional)\"),\n",
        "        gr.Image(type=\"pil\", label=\"Person 1 - Image 3 (optional)\"),\n",
        "        gr.Image(type=\"pil\", label=\"Person 2 - Image 1\"),\n",
        "        gr.Image(type=\"pil\", label=\"Person 2 - Image 2 (optional)\"),\n",
        "        gr.Image(type=\"pil\", label=\"Person 2 - Image 3 (optional)\"),\n",
        "        gr.Textbox(\n",
        "            label=\"Scene Description\",\n",
        "            placeholder=\"e.g., Two friends taking a sunset selfie flying to Dubai\",\n",
        "            value=\"Two friends taking a sunset selfie flying to Dubai\"\n",
        "        ),\n",
        "        gr.Slider(minimum=-1, maximum=10000, value=42, step=1, label=\"Seed (-1 for random)\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Generated Inflight Selfie\"),\n",
        "        gr.Textbox(label=\"Status\"),\n",
        "    ],\n",
        "    title=\"‚úàÔ∏è Inflight Selfie Generator\",\n",
        "    description=\"Upload photos of two people and describe your dream inflight selfie scene!\",\n",
        "    examples=[\n",
        "        [None, None, None, None, None, None, \"Two friends taking a sunset selfie flying to Dubai\", 42],\n",
        "        [None, None, None, None, None, None, \"Business class celebration with champagne\", 123],\n",
        "        [None, None, None, None, None, None, \"Night flight with city lights below\", 456],\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Launch\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "gradio_demo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 5: Export Models for Production"
      ],
      "metadata": {
        "id": "phase5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Export All Models\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"üì¶ Exporting models for production...\\n\")\n",
        "\n",
        "# Create export directory\n",
        "EXPORT_DIR = Path(\"./inflight_selfie_export\")\n",
        "EXPORT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Export scene planner\n",
        "if os.path.exists(\"scene_planner_lora\"):\n",
        "    print(\"  Copying scene planner LoRA weights...\")\n",
        "    shutil.copytree(\"scene_planner_lora\", EXPORT_DIR / \"scene_planner_lora\", dirs_exist_ok=True)\n",
        "    print(\"    ‚úì Scene planner exported\")\n",
        "\n",
        "# Create README\n",
        "readme_content = \"\"\"# Inflight Selfie Generator - Exported Models\n",
        "\n",
        "## Contents\n",
        "\n",
        "- `scene_planner_lora/` - Fine-tuned TinyLlama LoRA weights for scene planning\n",
        "\n",
        "## Base Models Required\n",
        "\n",
        "These will be downloaded automatically by the pipeline:\n",
        "\n",
        "1. **SDXL Base**: `stabilityai/stable-diffusion-xl-base-1.0`\n",
        "2. **IP-Adapter-FaceID**: `h94/IP-Adapter-FaceID`\n",
        "3. **TinyLlama Base**: `TinyLlama/TinyLlama-1.1B-Chat-v1.0`\n",
        "4. **InsightFace**: antelopev2 models\n",
        "\n",
        "## Usage\n",
        "\n",
        "1. Extract this archive\n",
        "2. Place in your project's `models/` directory\n",
        "3. The pipeline will automatically load the LoRA weights\n",
        "\n",
        "## Training Details\n",
        "\n",
        "- **Scene Planner**: TinyLlama + LoRA (r=16)\n",
        "- **Training Data**: 10 inflight scene examples\n",
        "- **Training Time**: ~20 minutes on T4 GPU\n",
        "- **Framework**: Unsloth + TRL\n",
        "\"\"\"\n",
        "\n",
        "with open(EXPORT_DIR / \"README.md\", \"w\") as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "print(\"\\n  Creating archive...\")\n",
        "!zip -r inflight_selfie_models.zip {EXPORT_DIR}\n",
        "\n",
        "print(\"\\n‚úÖ Export complete!\\n\")\n",
        "print(\"üì• Downloading archive...\")\n",
        "files.download(\"inflight_selfie_models.zip\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚ú® Models exported successfully!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "export_models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has:\n",
        "\n",
        "1. ‚úÖ Set up IP-Adapter-FaceID with SDXL\n",
        "2. ‚úÖ Fine-tuned TinyLlama for scene planning\n",
        "3. ‚úÖ Created complete generation pipeline\n",
        "4. ‚úÖ Provided testing and demo interfaces\n",
        "5. ‚úÖ Exported models for production\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Download the exported models zip\n",
        "2. Set up the FastAPI backend (see `server.py`)\n",
        "3. Build the Next.js frontend\n",
        "4. Deploy to production\n",
        "\n",
        "### Performance Tips\n",
        "\n",
        "- Use multiple face images per person for better results\n",
        "- Adjust `ip_adapter_scale` (0.5-0.8) to balance identity vs scene quality\n",
        "- Higher `guidance_scale` (8-10) for more prompt adherence\n",
        "- Use seeds for reproducible results\n",
        "\n",
        "### Credits\n",
        "\n",
        "- IP-Adapter-FaceID: Tencent AI Lab\n",
        "- Stable Diffusion XL: Stability AI\n",
        "- InsightFace: Jia Guo, Jiankang Deng\n",
        "- TinyLlama: Zhang et al.\n"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}
